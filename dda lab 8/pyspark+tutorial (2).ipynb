{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T06:53:41.353877Z",
     "start_time": "2020-07-03T06:53:41.343902Z"
    }
   },
   "source": [
    "# Apache Spark Basics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a) Basic Operations on Resilient Distributed Dataset (RDD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Perform rightOuterJoin and fullOuterJoin operations between a and b. Briefly explain your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:06:01.643526Z",
     "start_time": "2020-07-06T02:06:01.493271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\spark\\\\spark-2.3.2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:06:03.161958Z",
     "start_time": "2020-07-06T02:06:02.787580Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T06:53:44.307056Z",
     "start_time": "2020-07-03T06:53:43.569912Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T06:53:49.304381Z",
     "start_time": "2020-07-03T06:53:49.010046Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:06:17.545867Z",
     "start_time": "2020-07-06T02:06:17.539885Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing a\n",
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:06:19.627716Z",
     "start_time": "2020-07-06T02:06:19.622730Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing b\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:07:01.289126Z",
     "start_time": "2020-07-06T02:07:00.561070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('rdd', 1),\n",
       " ('python', 1),\n",
       " ('context', 1),\n",
       " ('create', 1),\n",
       " ('class', 1)]"
      ]
     },
     "execution_count": 1122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating RDDs from 'a' such that, that RDD is a key value pair\n",
    "def add_key(value):\n",
    "    return (value,1)\n",
    "rdd1 = sc.parallelize(a)\n",
    "rdd1 = rdd1.map(add_key)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:07:03.229520Z",
     "start_time": "2020-07-06T02:07:02.173309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('operation', 1),\n",
       " ('apache', 1),\n",
       " ('scala', 1),\n",
       " ('lambda', 1),\n",
       " ('parallel', 1),\n",
       " ('partition', 1)]"
      ]
     },
     "execution_count": 1123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating RDDs from 'b' such that, that RDD is a key value pair\n",
    "rdd2 = sc.parallelize(b)\n",
    "rdd2 = rdd2.map(add_key)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:07:03.991877Z",
     "start_time": "2020-07-06T02:07:03.955972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Right outer join\n",
    "roj = rdd1.rightOuterJoin(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:07:10.575529Z",
     "start_time": "2020-07-06T02:07:05.126757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', (None, 1)),\n",
       " ('parallel', (None, 1)),\n",
       " ('partition', (None, 1)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('lambda', (None, 1))]"
      ]
     },
     "execution_count": 1125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roj.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:07:17.249438Z",
     "start_time": "2020-07-06T02:07:11.598668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', (1, None)),\n",
       " ('class', (1, None)),\n",
       " ('scala', (None, 1)),\n",
       " ('parallel', (None, 1)),\n",
       " ('partition', (None, 1)),\n",
       " ('spark', (1, None)),\n",
       " ('rdd', (1, None)),\n",
       " ('context', (1, None)),\n",
       " ('create', (1, None)),\n",
       " ('operation', (None, 1)),\n",
       " ('apache', (None, 1)),\n",
       " ('lambda', (None, 1))]"
      ]
     },
     "execution_count": 1126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full outer join\n",
    "rdd1.fullOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T09:42:02.071646Z",
     "start_time": "2020-07-03T09:42:02.031736Z"
    }
   },
   "source": [
    "<b>Explanation</b>\n",
    " - The two inputs given are converted into two RDD's respectively such that each RDD is a key value pair with the key as the words in input and the value as 1.\n",
    " - Right outer join is then computed on both RDD's\n",
    " - Full outer join is also computed on both RDD's\n",
    " - 1 in the value of output represents the presence of the key in the joined table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using map and reduce functions to count how many times the character \"s\" appears in all a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:13:15.848794Z",
     "start_time": "2020-07-06T02:13:13.293090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "#function to count and return the number of occurences of 's' in its parameter\n",
    "def detect_s(x):\n",
    "    x = list(x)\n",
    "    c =0\n",
    "    for i in x:\n",
    "        if i == 's':\n",
    "            c+=1\n",
    "    return c\n",
    "\n",
    "rdd3 = sc.parallelize(a)\n",
    "rdd4 = sc.parallelize(b)\n",
    "#Combining RDD's for input's a and b\n",
    "newr = sc.union([rdd3,rdd4])\n",
    "#Mapping Input words containing 's' to the number of 's's in that word and thereby passing it to reduce\n",
    "rdd_s = newr.map(detect_s).filter(lambda x: x is not None).reduce(add)\n",
    "print(rdd_s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation</b>\n",
    " - Create RDD's from a and b\n",
    " - Combine the RDD's\n",
    " - Detect the number of 's's in a word and place it as the value in a key-value pair with the word being the key\n",
    " - Reduce the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using aggregate function to count how many times the character \"s\" appears in all a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T02:40:53.130377Z",
     "start_time": "2020-07-06T02:40:51.345512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "def detect_s(x):\n",
    "    x = list(x)\n",
    "    c =0\n",
    "    for i in x:\n",
    "        if i == 's':\n",
    "            c+=1\n",
    "    return c\n",
    "\n",
    "rdd3 = sc.parallelize(a)\n",
    "rdd4 = sc.parallelize(b)\n",
    "newr = sc.union([rdd3,rdd4])\n",
    "rdd_s = newr.filter(lambda x: 's' in x).map(detect_s).sum()\n",
    "print(rdd_s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Explanation</b>\n",
    " - Create RDD's from a and b\n",
    " - Combine the RDD's\n",
    " - Filter words with s's and pass them to a map function that maps the words with the number of 's's in that word.\n",
    " - Aggregate the results using sum() aggregate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b) Basic Operations on DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:29.636247Z",
     "start_time": "2020-07-06T03:45:29.519252Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:30.566381Z",
     "start_time": "2020-07-06T03:45:30.467378Z"
    }
   },
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "df = sqlContext.read.json('C:\\\\Users\\\\admin\\\\Downloads\\\\students.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Replace the null value(s) in column points by the mean of all points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:32.160207Z",
     "start_time": "2020-07-06T03:45:32.057191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.736842105263158\n",
      "3.3307007147839007\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "#calculating mean of points \n",
    "df_stats = df.select(\n",
    "    _mean(col('points')).alias(\"mean\"),\n",
    "    _stddev(col('points')).alias(\"std deviation\")\n",
    ").collect()\n",
    "\n",
    "print(df_stats[0][\"mean\"])\n",
    "mean = df_stats[0]['mean']\n",
    "std = df_stats[0]['std deviation']\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T14:54:34.871020Z",
     "start_time": "2020-07-03T14:54:34.865036Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:35.230804Z",
     "start_time": "2020-07-06T03:45:35.080258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filling null values with mean of all points\n",
    "df = df.na.fill(mean,['points'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:37.209436Z",
     "start_time": "2020-07-06T03:45:37.008420Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "#filling null values with values specified in the question\n",
    "df1 = df.na.fill(\"unknown\",\"dob\")\n",
    "df1.show()\n",
    "df1 = df1.na.fill(\"--\",\"last_name\")\n",
    "df1.show()\n",
    "df = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation for Part b) 1 and 2</b>\n",
    " - The null values in points are replaced by mean of the column points by using the mean sql function\n",
    " - The null values in dob and lastname are replaced by 'unknown' and '--' respectively using fill method of pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T05:48:24.359838Z",
     "start_time": "2020-07-04T05:48:24.342883Z"
    }
   },
   "source": [
    "# 3.In the dob column, there exist several formats of dates, e.g. October 14, 1983 and 26 December 1989. Let’s convert all the dates into DD-MM-YYYY format where DD, MM and YYYY are two digits for day, two digits for months and four digits for year respectively \n",
    "# 4.Insert a new column age and calculate the current age of all students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:40.337326Z",
     "start_time": "2020-07-06T03:45:39.564819Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id| age|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|Humanities and Art|14/10/1983|      Alan|      Joe|    10|   1|  37|\n",
      "|  Computer Science| 26/9/1980|    Martin|  Genberg|    17|   2|  40|\n",
      "|    Graphic Design| 12/6/1982|     Athur|   Watson|    16|   3|  38|\n",
      "|    Graphic Design|  5/4/1987|  Anabelle|  Sanberg|    12|   4|  33|\n",
      "|        Psychology| 1/11/1978|      Kira| Schommer|    11|   5|  42|\n",
      "|          Business| 17/2/1981| Christian|   Kiriam|    10|   6|  39|\n",
      "|  Machine Learning|  1/1/1984|   Barbara|  Ballard|    14|   7|  36|\n",
      "|     Deep Learning| 13/1/1978|      John|       --|    10|   8|  42|\n",
      "|  Machine Learning|26/12/1989|    Marcus|   Carson|    15|   9|  31|\n",
      "|           Physics|30/12/1987|     Marta|   Brooks|    11|  10|  33|\n",
      "|    Data Analytics| 12/6/1975|     Holly| Schwartz|    12|  11|  45|\n",
      "|  Computer Science|  2/7/1985|     April|    Black|    11|  12|  35|\n",
      "|  Computer Science| 22/7/1980|     Irene|  Bradley|    13|  13|  40|\n",
      "|        Psychology|  7/2/1986|      Mark|    Weber|    12|  14|  34|\n",
      "|       Informatics| 18/5/1987|     Rosie|   Norman|     9|  15|  33|\n",
      "|          Business| 10/8/1984|    Martin|   Steele|     7|  16|  36|\n",
      "|  Machine Learning|16/12/1990|     Colin| Martinez|     9|  17|  30|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|null|\n",
      "|          Business|  7/3/1980|   Darlene|    Mills|    19|  19|  40|\n",
      "|    Data Analytics|  2/6/1985|   Zachary|       --|    10|  20|  35|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "#function to change the format of the date to DD-MM-YYYY\n",
    "def changedateformat(value):\n",
    "    #dictionary with keys as months and the values as the month's number\n",
    "    months = {'January':1,'February':2,\"March\":3,\"April\":4,\"May\":5,\"June\":6,\"July\":7,\"August\":8,\"September\":9,\"October\":10,\"November\":11,\"December\":12,\"None\":\"--\"}\n",
    "    valu = str(value)\n",
    "    valu1 = str(valu)\n",
    "    v = valu.split()\n",
    "    print(type(value))\n",
    "    #regexes for detecting months, years, days from the dob column\n",
    "    regmonth = r\"([a-zA-Z]+)\"\n",
    "    regday = r\"(\\d+)\"\n",
    "    regyear = r\"([0-9]{4})\"\n",
    "    \n",
    "    #matching regexes with the dob column\n",
    "    matchmonth = re.search(regmonth, valu) \n",
    "    matchday = re.search(regday, valu)\n",
    "    matchyear = re.search(regyear, valu)\n",
    "\n",
    "    if(matchyear != None):\n",
    "        year = str(matchyear.group(0))\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "    if(matchday!=None):\n",
    "        day = str(matchday.group(0))\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "    #returning the result in DD_MM-YYYY format\n",
    "    if(matchmonth!=None and matchday!=None and matchyear!=None):\n",
    "        return day+\"/\"+str(months[str(matchmonth.group(0))])+\"/\"+year\n",
    "    \n",
    "    \n",
    "def computeage(value):\n",
    "    #matching year\n",
    "    regyear = r\"([0-9]{4})\"\n",
    "    matchyear = re.search(regyear, str(value))\n",
    "    #subtracting matched year from 2020\n",
    "    if(matchyear!=None):\n",
    "        return 2020-int(str(matchyear.group(0)))\n",
    "    return None\n",
    "#convert to a UDF Function by passing in the function and return type of function\n",
    "udfdate = F.udf(changedateformat, StringType())\n",
    "udfage = F.udf(computeage, StringType())\n",
    "df = df.withColumn(\"dob\", udfdate(\"dob\"))\n",
    "df = df.withColumn(\"age\",udfage(\"dob\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Explanation</b>\n",
    " - changedateformat() function takes in date of births in any format and changes it to DD/MM/YYYY format.\n",
    " - UserDefinedFunction library is used to process the dataframe using user written functions\n",
    " - computeage() function extracts year of birth from the dob column and calculates the age of the person if the dob is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:51.835513Z",
     "start_time": "2020-07-06T03:45:51.172217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id| age|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|Humanities and Art|14/10/1983|      Alan|      Joe|    10|   1|  37|\n",
      "|  Computer Science| 26/9/1980|    Martin|  Genberg|    17|   2|  40|\n",
      "|    Graphic Design| 12/6/1982|     Athur|   Watson|    16|   3|  38|\n",
      "|    Graphic Design|  5/4/1987|  Anabelle|  Sanberg|    12|   4|  33|\n",
      "|        Psychology| 1/11/1978|      Kira| Schommer|    11|   5|  42|\n",
      "|          Business| 17/2/1981| Christian|   Kiriam|    10|   6|  39|\n",
      "|  Machine Learning|  1/1/1984|   Barbara|  Ballard|    14|   7|  36|\n",
      "|     Deep Learning| 13/1/1978|      John|       --|    10|   8|  42|\n",
      "|  Machine Learning|26/12/1989|    Marcus|   Carson|    15|   9|  31|\n",
      "|           Physics|30/12/1987|     Marta|   Brooks|    11|  10|  33|\n",
      "|    Data Analytics| 12/6/1975|     Holly| Schwartz|    12|  11|  45|\n",
      "|  Computer Science|  2/7/1985|     April|    Black|    11|  12|  35|\n",
      "|  Computer Science| 22/7/1980|     Irene|  Bradley|    13|  13|  40|\n",
      "|        Psychology|  7/2/1986|      Mark|    Weber|    12|  14|  34|\n",
      "|       Informatics| 18/5/1987|     Rosie|   Norman|     9|  15|  33|\n",
      "|          Business| 10/8/1984|    Martin|   Steele|     7|  16|  36|\n",
      "|  Machine Learning|16/12/1990|     Colin| Martinez|     9|  17|  30|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|null|\n",
      "|          Business|  7/3/1980|   Darlene|    Mills|    19|  19|  40|\n",
      "|    Data Analytics|  2/6/1985|   Zachary|       --|    10|  20|  35|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T11:44:46.639735Z",
     "start_time": "2020-07-04T11:44:46.534003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T11:33:58.149908Z",
     "start_time": "2020-07-04T11:33:58.143922Z"
    }
   },
   "source": [
    "# 5.Let’s consider granting some points for good performed students in the class. For each student, if his point is larger than 1 standard deviation of all points, then we update his current point to 20, which is the maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T03:45:55.102904Z",
     "start_time": "2020-07-06T03:45:54.429772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id| age|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "|Humanities and Art|14/10/1983|      Alan|      Joe|    10|   1|  37|\n",
      "|  Computer Science| 26/9/1980|    Martin|  Genberg|    20|   2|  40|\n",
      "|    Graphic Design| 12/6/1982|     Athur|   Watson|    20|   3|  38|\n",
      "|    Graphic Design|  5/4/1987|  Anabelle|  Sanberg|    12|   4|  33|\n",
      "|        Psychology| 1/11/1978|      Kira| Schommer|    11|   5|  42|\n",
      "|          Business| 17/2/1981| Christian|   Kiriam|    10|   6|  39|\n",
      "|  Machine Learning|  1/1/1984|   Barbara|  Ballard|    14|   7|  36|\n",
      "|     Deep Learning| 13/1/1978|      John|       --|    10|   8|  42|\n",
      "|  Machine Learning|26/12/1989|    Marcus|   Carson|    15|   9|  31|\n",
      "|           Physics|30/12/1987|     Marta|   Brooks|    11|  10|  33|\n",
      "|    Data Analytics| 12/6/1975|     Holly| Schwartz|    12|  11|  45|\n",
      "|  Computer Science|  2/7/1985|     April|    Black|    11|  12|  35|\n",
      "|  Computer Science| 22/7/1980|     Irene|  Bradley|    13|  13|  40|\n",
      "|        Psychology|  7/2/1986|      Mark|    Weber|    12|  14|  34|\n",
      "|       Informatics| 18/5/1987|     Rosie|   Norman|     9|  15|  33|\n",
      "|          Business| 10/8/1984|    Martin|   Steele|     7|  16|  36|\n",
      "|  Machine Learning|16/12/1990|     Colin| Martinez|     9|  17|  30|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|null|\n",
      "|          Business|  7/3/1980|   Darlene|    Mills|    20|  19|  40|\n",
      "|    Data Analytics|  2/6/1985|   Zachary|       --|    10|  20|  35|\n",
      "+------------------+----------+----------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn('points',when(df.points > df_stats[0][\"mean\"]+df_stats[0][\"std deviation\"] ,20).otherwise(df.points))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation</b>\n",
    " - Calculate mean+standard deviation for each point and replace points with 20 if they have more value than the sum mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T15:30:02.580357Z",
     "start_time": "2020-07-03T15:30:02.550435Z"
    }
   },
   "source": [
    "# 6. Create a histogram on the new points created in the task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:40:30.102821Z",
     "start_time": "2020-07-06T06:40:29.867178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[10, 20, 20, 12, 11, 10, 14, 10, 15, 11, 12, 11, 13, 12, 9, 7, 9, 6, 20, 10]\n"
     ]
    }
   ],
   "source": [
    "# Converting column points to list \n",
    "points = df.select('points').collect()\n",
    "print(type(points))\n",
    "points = [int(i.points) for i in points]\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:41:02.497535Z",
     "start_time": "2020-07-06T06:41:02.340653Z"
    }
   },
   "outputs": [],
   "source": [
    "#plotting a histogram\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.hist(points)\n",
    "plt.xlabel('points')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Ex 1. Part b) 6.')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ex 2: Manipulating Recommender Dataset with Apache Spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:56:41.174373Z",
     "start_time": "2020-07-06T04:56:41.041799Z"
    }
   },
   "outputs": [],
   "source": [
    "#reading the dataset and preprocessing\n",
    "tags = sqlContext.read.format(\"csv\").option(\"delimiter\",\":\").load('C:\\\\Users\\\\admin\\\\moviedataset\\\\ml-10M100K\\\\tags.dat')\n",
    "tags = tags.drop('_c1').drop('_c3').drop('_c5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:56:43.199515Z",
     "start_time": "2020-07-06T04:56:43.194572Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:56:45.163312Z",
     "start_time": "2020-07-06T04:56:45.138378Z"
    }
   },
   "outputs": [],
   "source": [
    "#ordering by users and time\n",
    "tags = tags.orderBy('_c0','_c6', ascending=True)\n",
    "\n",
    "Windowspec = Window.orderBy('_c0')\n",
    "#Windowspec = Windowspec.orderBy('_c6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:56:48.314094Z",
     "start_time": "2020-07-06T04:56:48.298136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a new column with value of column '_c6' from t-1\n",
    "previous_time = tags.withColumn('previous_time', F.lag(tags['_c6']).over(Windowspec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:56:59.288827Z",
     "start_time": "2020-07-06T04:56:49.708382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+----------+-------------+\n",
      "|  _c0|  _c2|             _c4|       _c6|previous_time|\n",
      "+-----+-----+----------------+----------+-------------+\n",
      "| 1000|  277|children's story|1188533111|         null|\n",
      "| 1000| 1994|    sci-fi. dark|1188533136|   1188533111|\n",
      "| 1000| 5377|         romance|1188533150|   1188533136|\n",
      "| 1000| 7147|    family bonds|1188533161|   1188533150|\n",
      "| 1000|  362|animated classic|1188533171|   1188533161|\n",
      "| 1000|  276|          family|1188533235|   1188533171|\n",
      "|10003|42013|        Passable|1150432435|   1188533235|\n",
      "|10003|51662|  FIOS on demand|1207953326|   1150432435|\n",
      "|10003|54997|  FIOS on demand|1207953335|   1207953326|\n",
      "|10003|55765|  FIOS on demand|1207953342|   1207953335|\n",
      "|10003|55363|  FIOS on demand|1207953420|   1207953342|\n",
      "|10003|56152|  FIOS on demand|1207953526|   1207953420|\n",
      "|10003|55116|  FIOS on demand|1207953636|   1207953526|\n",
      "|10003|56174|  FIOS on demand|1207953670|   1207953636|\n",
      "|10003|55176|  FIOS on demand|1207953755|   1207953670|\n",
      "|10003|55247|  FIOS on demand|1207953756|   1207953755|\n",
      "|10003|54881|  FIOS on demand|1207953758|   1207953756|\n",
      "|10003|55820|  FIOS on demand|1207953873|   1207953758|\n",
      "|10003|53123|  FIOS on demand|1207953875|   1207953873|\n",
      "|10003|53550|  FIOS on demand|1207953937|   1207953875|\n",
      "+-----+-----+----------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:57:18.648082Z",
     "start_time": "2020-07-06T04:57:18.638108Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating time difference between values in column _c6 at time t-1 and t\n",
    "result = previous_time.withColumn('time_difference', (previous_time['_c6']-previous_time['previous_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T04:57:27.954992Z",
     "start_time": "2020-07-06T04:57:20.137253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+----------+-------------+---------------+\n",
      "|  _c0|  _c2|             _c4|       _c6|previous_time|time_difference|\n",
      "+-----+-----+----------------+----------+-------------+---------------+\n",
      "| 1000|  277|children's story|1188533111|         null|           null|\n",
      "| 1000| 1994|    sci-fi. dark|1188533136|   1188533111|           25.0|\n",
      "| 1000| 5377|         romance|1188533150|   1188533136|           14.0|\n",
      "| 1000| 7147|    family bonds|1188533161|   1188533150|           11.0|\n",
      "| 1000|  362|animated classic|1188533171|   1188533161|           10.0|\n",
      "| 1000|  276|          family|1188533235|   1188533171|           64.0|\n",
      "|10003|42013|        Passable|1150432435|   1188533235|     -3.81008E7|\n",
      "|10003|51662|  FIOS on demand|1207953326|   1150432435|    5.7520891E7|\n",
      "|10003|54997|  FIOS on demand|1207953335|   1207953326|            9.0|\n",
      "|10003|55765|  FIOS on demand|1207953342|   1207953335|            7.0|\n",
      "|10003|55363|  FIOS on demand|1207953420|   1207953342|           78.0|\n",
      "|10003|56152|  FIOS on demand|1207953526|   1207953420|          106.0|\n",
      "|10003|55116|  FIOS on demand|1207953636|   1207953526|          110.0|\n",
      "|10003|56174|  FIOS on demand|1207953670|   1207953636|           34.0|\n",
      "|10003|55176|  FIOS on demand|1207953755|   1207953670|           85.0|\n",
      "|10003|55247|  FIOS on demand|1207953756|   1207953755|            1.0|\n",
      "|10003|54881|  FIOS on demand|1207953758|   1207953756|            2.0|\n",
      "|10003|55820|  FIOS on demand|1207953873|   1207953758|          115.0|\n",
      "|10003|53123|  FIOS on demand|1207953875|   1207953873|            2.0|\n",
      "|10003|53550|  FIOS on demand|1207953937|   1207953875|           62.0|\n",
      "+-----+-----+----------------+----------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T05:06:33.180560Z",
     "start_time": "2020-07-06T05:06:33.145687Z"
    }
   },
   "outputs": [],
   "source": [
    "prev_time= 0\n",
    "prev_user = 0\n",
    "session = 0\n",
    "#assigns session based on time difference\n",
    "def assign_session(time, user):\n",
    "    global prev_time\n",
    "    global prev_user\n",
    "    global session\n",
    "    if(user!=prev_user):\n",
    "        time = float(\"-inf\")\n",
    "    if(time== None or time > 1800 or user!=prev_user or time< -1800):\n",
    "        session+=1\n",
    "    prev_user = user\n",
    "    return session\n",
    "udfsession = F.udf(assign_session, StringType())\n",
    "\n",
    "tags_session = result.withColumn(\"session\",udfsession(\"time_difference\",'_c0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tagging session for each user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T05:06:45.762422Z",
     "start_time": "2020-07-06T05:06:36.183800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------+----------+-------------+---------------+-------+\n",
      "|  _c0|  _c2|             _c4|       _c6|previous_time|time_difference|session|\n",
      "+-----+-----+----------------+----------+-------------+---------------+-------+\n",
      "| 1000|  277|children's story|1188533111|         null|           null|      1|\n",
      "| 1000| 1994|    sci-fi. dark|1188533136|   1188533111|           25.0|      1|\n",
      "| 1000| 5377|         romance|1188533150|   1188533136|           14.0|      1|\n",
      "| 1000| 7147|    family bonds|1188533161|   1188533150|           11.0|      1|\n",
      "| 1000|  362|animated classic|1188533171|   1188533161|           10.0|      1|\n",
      "| 1000|  276|          family|1188533235|   1188533171|           64.0|      1|\n",
      "|10003|42013|        Passable|1150432435|   1188533235|     -3.81008E7|      2|\n",
      "|10003|51662|  FIOS on demand|1207953326|   1150432435|    5.7520891E7|      3|\n",
      "|10003|54997|  FIOS on demand|1207953335|   1207953326|            9.0|      3|\n",
      "|10003|55765|  FIOS on demand|1207953342|   1207953335|            7.0|      3|\n",
      "|10003|55363|  FIOS on demand|1207953420|   1207953342|           78.0|      3|\n",
      "|10003|56152|  FIOS on demand|1207953526|   1207953420|          106.0|      3|\n",
      "|10003|55116|  FIOS on demand|1207953636|   1207953526|          110.0|      3|\n",
      "|10003|56174|  FIOS on demand|1207953670|   1207953636|           34.0|      3|\n",
      "|10003|55176|  FIOS on demand|1207953755|   1207953670|           85.0|      3|\n",
      "|10003|55247|  FIOS on demand|1207953756|   1207953755|            1.0|      3|\n",
      "|10003|54881|  FIOS on demand|1207953758|   1207953756|            2.0|      3|\n",
      "|10003|55820|  FIOS on demand|1207953873|   1207953758|          115.0|      3|\n",
      "|10003|53123|  FIOS on demand|1207953875|   1207953873|            2.0|      3|\n",
      "|10003|53550|  FIOS on demand|1207953937|   1207953875|           62.0|      3|\n",
      "+-----+-----+----------------+----------+-------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags_session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation</b>\n",
    " - Preprocess the dataset to have an additional column that dscribes the time difference between two tags of a user\n",
    " - Assign a new session if the time difference is greater than 30 minutes = 1800 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Frequency of tagging for each user session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T05:07:46.012578Z",
     "start_time": "2020-07-06T05:07:45.991600Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_tag = tags_session.orderBy('session').groupBy('session').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T05:07:56.871538Z",
     "start_time": "2020-07-06T05:07:47.217850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|session|count|\n",
      "+-------+-----+\n",
      "|      1|    6|\n",
      "|     10|    1|\n",
      "|    100|    1|\n",
      "|   1000|    6|\n",
      "|  10000|    5|\n",
      "|  10001|    8|\n",
      "|  10002|    1|\n",
      "|  10003|    1|\n",
      "|  10004|    7|\n",
      "|  10005|    3|\n",
      "|  10006|    2|\n",
      "|  10007|    2|\n",
      "|  10008|    1|\n",
      "|  10009|    1|\n",
      "|   1001|    1|\n",
      "|  10010|    1|\n",
      "|  10011|    2|\n",
      "|  10012|    1|\n",
      "|  10013|    2|\n",
      "|  10014|    1|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq_tag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation</b>\n",
    " - Perform groupBy on session attribute and count() such that the frequency of tagging within each user session is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Mean and standard deviation of the tagging frequency for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T05:43:26.556651Z",
     "start_time": "2020-07-06T05:43:18.957781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+\n",
      "|  _c0|session|count|\n",
      "+-----+-------+-----+\n",
      "| 1000|      1|    6|\n",
      "|10003|      2|    1|\n",
      "|10003|      3|   18|\n",
      "|10003|      4|   38|\n",
      "|10020|      5|    2|\n",
      "|10025|      6|    1|\n",
      "|10032|     10|    1|\n",
      "|10032|     11|    4|\n",
      "|10032|     12|    1|\n",
      "|10032|     13|    1|\n",
      "|10032|     14|    4|\n",
      "|10032|     15|    1|\n",
      "|10032|     16|    1|\n",
      "|10032|     17|    1|\n",
      "|10032|     18|    1|\n",
      "|10032|      7|   39|\n",
      "|10032|      8|    1|\n",
      "|10032|      9|    1|\n",
      "|10051|     19|    1|\n",
      "|10058|     20|   35|\n",
      "+-----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_user = tags_session.groupBy('_c0','session').count().orderBy('_c0','session')\n",
    "per_user.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The above table shows the number of tags performed for each session by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:20:39.490573Z",
     "start_time": "2020-07-06T06:18:37.440583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  _c0|        avg(count)|\n",
      "+-----+------------------+\n",
      "| 1000|               6.0|\n",
      "|10003|              19.0|\n",
      "|10020|               2.0|\n",
      "|10025|               1.0|\n",
      "|10032| 4.666666666666667|\n",
      "|10051|               1.0|\n",
      "|10058|25.333333333333332|\n",
      "|10059|               2.5|\n",
      "|10064|               1.0|\n",
      "|10084|              3.75|\n",
      "|10094|               2.0|\n",
      "| 1010|               4.0|\n",
      "|10117|               1.5|\n",
      "|10125|              21.0|\n",
      "|10132|            1.5625|\n",
      "|10154|               8.0|\n",
      "|10167|               1.0|\n",
      "| 1017|               7.0|\n",
      "|10181|              11.0|\n",
      "|10191|2.6666666666666665|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#mean of tagging frequency of each user\n",
    "per_user_mean = per_user.groupBy('_c0').mean().orderBy('_c0')\n",
    "per_user_mean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explanation </b>\n",
    " - This table calculates the mean of tagging frequency of each user by performing groupBy on user and then calculating the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:05:05.143497Z",
     "start_time": "2020-07-06T06:02:39.489502Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|  _c0|     stddev(count)|\n",
      "+-----+------------------+\n",
      "|11563|               NaN|\n",
      "| 1436|               NaN|\n",
      "|17427|0.7071067811865476|\n",
      "| 2136|               NaN|\n",
      "|23318|               NaN|\n",
      "|28473|               0.0|\n",
      "| 2904|               NaN|\n",
      "|29549|               NaN|\n",
      "|32812|1.4142135623730951|\n",
      "|38271|               NaN|\n",
      "|39917|               NaN|\n",
      "|42688|               NaN|\n",
      "|44446|               NaN|\n",
      "|48370|               0.0|\n",
      "| 5325|               NaN|\n",
      "|57051|               NaN|\n",
      "|57112|               NaN|\n",
      "|57464|               NaN|\n",
      "|58744|               NaN|\n",
      "| 5925|               0.0|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#std of tagging frequency of each user\n",
    "per_user.groupBy('_c0').agg({'count':'std'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:33:36.324797Z",
     "start_time": "2020-07-06T06:33:36.317814Z"
    }
   },
   "source": [
    "<b>Explanation </b>\n",
    " - This table calculates the std of tagging frequency of each user by performing groupBy on user and then calculating the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mean and standard deviation of the tagging frequency across users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:20:51.921555Z",
     "start_time": "2020-07-06T06:20:39.493527Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_stats = freq_tag.select(\n",
    "    _mean(col('count')).alias(\"mean\"),\n",
    "    _stddev(col('count')).alias(\"std deviation\")\n",
    ").collect()\n",
    "\n",
    "freq_mean = (freq_stats[0][\"mean\"])\n",
    "freq_std = (freq_stats[0][\"std deviation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:21:12.044268Z",
     "start_time": "2020-07-06T06:21:12.038281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.269596589045589 20.519115697996106\n"
     ]
    }
   ],
   "source": [
    "print(freq_mean, freq_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Provide the list of users with a mean tagging frequency within the two standard deviation from the mean frequency of all users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:34:13.201086Z",
     "start_time": "2020-07-06T06:34:12.976090Z"
    }
   },
   "outputs": [],
   "source": [
    "m = per_user_mean.select('_c0').filter(per_user_mean['avg(count)']<= freq_mean+2*freq_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T06:36:23.229766Z",
     "start_time": "2020-07-06T06:34:14.459267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  _c0|\n",
      "+-----+\n",
      "| 1000|\n",
      "|10003|\n",
      "|10020|\n",
      "|10025|\n",
      "|10032|\n",
      "|10051|\n",
      "|10058|\n",
      "|10059|\n",
      "|10064|\n",
      "|10084|\n",
      "|10094|\n",
      "| 1010|\n",
      "|10117|\n",
      "|10125|\n",
      "|10132|\n",
      "|10154|\n",
      "|10167|\n",
      "| 1017|\n",
      "|10181|\n",
      "|10191|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Explanation</b>\n",
    " - Filtering users whose per user mean is within two stds from the mean across all users"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
